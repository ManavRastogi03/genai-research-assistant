# ğŸ§  Task: Smart Assistant for Research Summarization

---

## ğŸ¯ Objective

Build an AI-powered assistant that can read and deeply understand user-uploaded documents (PDF or TXT). The assistant should support both free-form question answering and logical reasoning, ensuring all answers are grounded in the source content.

---

## ğŸ“š Problem Statement

Reading lengthy documents such as research papers or technical manuals is time-consuming. Simple summarizers or keyword search tools lack comprehension. This tool bridges that gap by enabling users to:

- Ask intelligent questions requiring inference
- Receive contextual answers with source justification
- Be tested with logic-based questions auto-generated from the document

---

## âœ… Functional Requirements

### 1. ğŸ“„ Document Upload
- Accepts `.pdf` and `.txt` files
- Assumes input is structured English (e.g., reports, research papers)

### 2. ğŸ’¬ Interaction Modes

#### a. Ask Anything
- Free-form user questions
- Assistant answers using contextual evidence from the document

#### b. Challenge Me
- Generates 3 logic/inference-based questions
- Accepts user answers
- Evaluates and provides feedback + justification

### 3. ğŸ“Œ Contextual Understanding
- All responses are grounded in uploaded content
- No hallucination or fabricated responses
- Each answer includes reference (e.g., â€œSupported by paragraph 3 of section 1â€)

### 4. ğŸ§  Auto Summary (â‰¤150 words)
- After upload, a concise summary of the document is generated instantly

### 5. ğŸŒ Application Architecture
- Web-based interface running locally
- Frontend: Streamlit
- Backend: FastAPI
- Focus on seamless UX and smooth flow

---

## âœ¨ Bonus Features (Implemented)

- ğŸ” Memory Handling â€“ Supports follow-up questions with context awareness
- ğŸ” Answer Highlighting â€“ Shows source snippet backing each answer

---

## ğŸ§± Architecture / Reasoning Flow

```mermaid
graph TD
    A[ğŸ“„ Upload PDF/TXT] --> B[Text Extractor]
    B --> C[Text Cleaning + Chunking]
    C --> D[Vector Index (FAISS)]
    C --> E[Auto Summary Generator (LLM)]
    D --> F{User Mode}
    F -->|Ask Anything| G[Retriever â†’ LLM â†’ Answer + Justification]
    F -->|Challenge Me| H[Question Generator â†’ Answer Evaluation â†’ Feedback]
```

- Uses Retriever-Augmented Generation (RAG)
- Top-k relevant chunks passed to the model to reduce hallucinations

---

## ğŸ›  Tech Stack

| Layer         | Technology         |
|---------------|--------------------|
| UI            | Streamlit          |
| Backend       | FastAPI            |
| LLM           | Gemini API / GPT-4 |
| Chunking      | LangChain          |
| Vector Search | FAISS / ChromaDB   |
| Parsing       | PyMuPDF            |
| Memory        | LangChain Memory   |

---

## ğŸ§ª Getting Started

### 1ï¸âƒ£ Clone the Repository

```bash
git clone https://github.com/ManavRastogi03/genai-research-assistant.git
cd genai-research-assistant
```

### 2ï¸âƒ£ Create a Virtual Environment

```bash
python -m venv .venv
source .venv/bin/activate  # Windows: .venv\Scripts\activate
```

### 3ï¸âƒ£ Install Dependencies

```bash
pip install -r requirements.txt
```

### 4ï¸âƒ£ Add Your API Key

Create a `.env` file:

```env
GEMINI_API_KEY=your_gemini_api_key_here
```

### 5ï¸âƒ£ Run the App

```bash
# Start backend
cd backend
uvicorn main:app --reload

# Start frontend
cd ../fronted
streamlit run app.py
```

---

## ğŸ“ Project Structure

```
genai-research-assistant/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ main.py
â”‚   â”œâ”€â”€ config.py
â”‚   â”œâ”€â”€ document_processor.py
â”‚   â”œâ”€â”€ llm_service.py
â”‚   â”œâ”€â”€ api_models.py
â”‚   â””â”€â”€ embeddings.py
â”‚
â”œâ”€â”€ fronted/
â”‚   â””â”€â”€ app.py
â”‚
â”œâ”€â”€ utils/
â”‚   â””â”€â”€ text_utils.py
â”‚
â”œâ”€â”€ static/
â”‚   â””â”€â”€ style.css
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ uploads/
â”‚
â”œâ”€â”€ .env.example
â”œâ”€â”€ .gitignore
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## ğŸ“ Example Use Case

- Upload a PDF research paper.
- Read the auto-generated summary.
- Ask: **"What were the challenges discussed in section 2?"**
- Get a cited answer like:  
  _â€œAs per paragraph 2 of Section 2...â€_
- Try the â€œChallenge Meâ€ quiz to test your understanding.

---

## ğŸ“Š Evaluation Criteria

| Category                            | Weight |
|-------------------------------------|--------|
| âœ… Response Quality + Justification  | 30%    |
| ğŸ§  Challenge Me Functionality        | 20%    |
| ğŸ¨ UI/UX & Smooth Flow               | 15%    |
| ğŸ§¹ Code Structure & Documentation    | 15%    |
| ğŸŒŸ Creativity / Bonus Features       | 10%    |
| ğŸš« Minimal Hallucination / Context   | 10%    |

---

---

## ğŸ“œ License

MIT Â© [ManavRastogi03](https://github.com/ManavRastogi03)
